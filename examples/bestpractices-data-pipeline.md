# Claude Code Best Practices - Data Pipeline Example

This is an EXAMPLE of a customized best practices document for a data pipeline project. 
It shows how the general template should be adapted for a specific project.

## ğŸ¯ Project Overview
**Project**: Customer Analytics Pipeline  
**Description**: ETL pipeline for processing customer behavior data from multiple sources  
**Technology Stack**: Python, Apache Airflow, BigQuery, Pandas  
**Deployment Platform**: Google Cloud Platform  
**Database/Storage**: BigQuery (project: analytics-prod)  

## ğŸ“ Project Structure (MANDATORY)
```
customer-analytics-pipeline/
â”œâ”€â”€ src/                              â† Production code ONLY
â”‚   â”œâ”€â”€ controllers/                  â† Main orchestration logic
â”‚   â”‚   â””â”€â”€ customer_data_controller.py
â”‚   â”œâ”€â”€ pipelines/                    â† Data processing pipelines
â”‚   â”‚   â”œâ”€â”€ extract_pipeline.py
â”‚   â”‚   â”œâ”€â”€ transform_pipeline.py
â”‚   â”‚   â””â”€â”€ load_pipeline.py
â”‚   â”œâ”€â”€ calculators/                  â† Business logic calculations
â”‚   â”‚   â”œâ”€â”€ calculate_ltv.py
â”‚   â”‚   â””â”€â”€ calculate_churn_score.py
â”‚   â”œâ”€â”€ utils/                        â† Reusable utilities
â”‚   â”‚   â”œâ”€â”€ bigquery_helpers.py
â”‚   â”‚   â”œâ”€â”€ data_validators.py
â”‚   â”‚   â””â”€â”€ log_writer.py
â”‚   â””â”€â”€ schemas/                      â† Data schemas
â”‚       â””â”€â”€ customer_schema.py
â”œâ”€â”€ tests/                            â† All test files
â”‚   â”œâ”€â”€ unit/
â”‚   â”œâ”€â”€ integration/
â”‚   â””â”€â”€ fixtures/
â”œâ”€â”€ experiments/                      â† Research & temporary code
â”‚   â”œâ”€â”€ sandbox/
â”‚   â””â”€â”€ 2025-01-15-new-metric-exploration/
â”œâ”€â”€ config/                           â† Configuration files
â”‚   â”œâ”€â”€ airflow_dags/
â”‚   â”œâ”€â”€ settings.yaml
â”‚   â””â”€â”€ credentials/
â”œâ”€â”€ docs/                             â† Documentation
â”‚   â”œâ”€â”€ claude/
â”‚   â””â”€â”€ customer-analytics-pipeline-bestpractices.md
â”œâ”€â”€ scripts/                          â† Utility scripts & runners
â”‚   â”œâ”€â”€ run_daily_pipeline.py
â”‚   â”œâ”€â”€ backfill_historical_data.py
â”‚   â””â”€â”€ verify_data_quality.py
â””â”€â”€ data/                             â† Sample data for testing
    â””â”€â”€ samples/
```

## ğŸš¨ CRITICAL RULES for Claude Code

[Same as template - these don't change]

## ğŸ“ File Naming Conventions

### Production Code (src/):
- `customer_data_controller.py` âœ…
- `calculate_ltv.py` âœ…
- `extract_pipeline.py` âœ…

### Scripts (scripts/):
- `run_daily_pipeline.py` âœ…
- `backfill_historical_data.py` âœ…
- `check_pipeline_status.py` âœ…

## ğŸ”„ Workflow Guidelines

### Starting New Data Source Integration:
**USER**: "Add integration for Shopify data"  
**CLAUDE**: Creates files in experiments/2025-01-21-shopify-integration/  
           Tests connection and data structure first

### Updating Pipeline Logic:
**USER**: "Update the LTV calculation to include new factors"  
**CLAUDE**: Modifies src/calculators/calculate_ltv.py  
           Updates corresponding tests in tests/unit/

## ğŸ“¦ Import Structure

```python
# Standard library imports
import json
import os
from datetime import datetime
from pathlib import Path

# Third-party imports
import pandas as pd
from google.cloud import bigquery
from apache_airflow import DAG

# Internal imports
from src.utils.bigquery_helpers import create_table, load_data
from src.calculators.calculate_ltv import calculate_customer_ltv
from src.pipelines.extract_pipeline import extract_shopify_data
```

## ğŸ—ƒï¸ Database & Table Standards

### BigQuery Tables:
- `raw_customer_events` - Raw event data
- `processed_customer_metrics` - Calculated metrics
- `customer_ltv_scores` - LTV calculations
- `customer_segments` - Segmentation results

### Table Naming Convention:
- `raw_[source]_[entity]` for raw data
- `processed_[entity]_[metric]` for processed data
- `tmp_[purpose]_[date]` for temporary tables

### Customer ID Format:
- Always use format: `CUST-XXXXXXXX`
- Source system IDs mapped in `customer_id_mapping` table

## ğŸ§ª Testing Guidelines

### Data Quality Tests:
```bash
# Run data quality checks
python scripts/verify_data_quality.py --date 2025-01-21

# Test pipeline components
pytest tests/unit/test_calculate_ltv.py
```

### Integration Tests:
- Test with sample data first
- Verify BigQuery permissions
- Check Airflow DAG syntax

## ğŸš€ Key Processes

### 1. Daily Pipeline Run:
```bash
# Run full pipeline
python scripts/run_daily_pipeline.py --date 2025-01-21
```

### 2. Historical Backfill:
```bash
# Backfill specific date range
python scripts/backfill_historical_data.py --start 2025-01-01 --end 2025-01-20
```

### 3. Data Quality Monitoring:
```bash
# Check today's data quality
python scripts/verify_data_quality.py
```

[Rest of template sections customized for data pipeline context...]

---

**Last Updated**: 2025-01-21  
**Project**: Customer Analytics Pipeline  
**Repository**: customer-analytics-pipeline  
**Maintained By**: Data Engineering Team
